{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc615f9-37fc-490d-80c2-b3edd8137aa9",
   "metadata": {},
   "source": [
    "Decision Transformer: Reinforcement Learning via Sequence Modeling\n",
    "Recreated by : Austin Runkle, \n",
    "\n",
    "In this project we will be implementing the decision transformer and comparing its preformance\n",
    "to an existing RL model TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3d7b5-271c-4fb3-9bb7-6f679f7fba04",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14b4b9-5c8d-45c0-b9d7-df8ead470e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c203e-196d-44b1-a44f-08a6dc54a674",
   "metadata": {},
   "source": [
    "Decision Transformer - For Continuous Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5de3b-6897-4018-b4e7-55cdbd0f9223",
   "metadata": {},
   "source": [
    "Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559e7a0-8f1c-41b4-bbd7-481815a26034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ebcbbb-ce71-4d33-97d4-d48b6e66ba2e",
   "metadata": {},
   "source": [
    "Decision Transformer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f4ac4-ecac-4366-8bbb-f2b8de6c2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R, s, a, t: returns -to -go , states , actions , or timesteps\n",
    "# K: context length ( length of each input to DecisionTransformer )\n",
    "# transformer : transformer with causal masking (GPT)\n",
    "# embed_s , embed_a , embed_R : linear embedding layers\n",
    "# embed_t : learned episode positional embedding\n",
    "# pred_a : linear action prediction layer\n",
    "# main model\n",
    "def DecisionTransformer (R , s , a , t ):\n",
    "    # compute embeddings for tokens\n",
    "    pos_embedding = embed_t ( t ) # per - timestep ( note : not per - token )\n",
    "    s_embedding = embed_s ( s ) + pos_embedding\n",
    "    a_embedding = embed_a ( a ) + pos_embedding\n",
    "    R_embedding = embed_R ( R ) + pos_embedding\n",
    "    # interleave tokens as (R_1 , s_1 , a_1 , ... , R_K , s_K )\n",
    "    input_embeds = stack ( R_embedding , s_embedding , a_embedding )\n",
    "    # use transformer to get hidden states\n",
    "    hidden_states = transformer ( input_embeds = input_embeds )\n",
    "    # select hidden states for action prediction tokens\n",
    "    a_hidden = unstack ( hidden_states ). actions\n",
    "    # predict action\n",
    "    return pred_a ( a_hidden )\n",
    "# training loop\n",
    "for (R , s , a , t ) in dataloader : # dims : ( batch_size , K, dim )\n",
    "    a_preds = DecisionTransformer (R , s , a , t )\n",
    "    loss = mean (( a_preds - a )**2) # L2 loss for continuous actions\n",
    "    optimizer . zero_grad (); loss . backward (); optimizer . step ()\n",
    "# evaluation loop\n",
    "target_return = 1 # for instance , expert - level return\n",
    "R , s , a , t , done = [ target_return ] , [ env . reset ()] , [] , [1] , False\n",
    "while not done : # autoregressive generation / sampling\n",
    "    # sample next action\n",
    "    action = DecisionTransformer (R , s , a , t )[ -1] # for cts actions\n",
    "    new_s , r , done , _ = env . step ( action )\n",
    "    # append new tokens to sequence\n",
    "    R = R + [ R [ -1] - r] # decrement returns -to -go with reward\n",
    "    s , a , t = s + [ new_s ] , a + [ action ] , t + [ len ( R )]\n",
    "    R , s , a , t = R [ - K :] , ... # only keep context length of K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9c808-a180-4416-b467-440886fcf2b0",
   "metadata": {},
   "source": [
    "Neural Network for Q Learning Atari using convolution neural network\n",
    "https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html <- Building Neural Networks\n",
    "https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html <- training classifier\n",
    "https://arxiv.org/pdf/1312.5602 <- confusion matrix sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e55e9d-9f57-4e6d-b007-388ff5119352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution neural network to work with atari\n",
    "class QLearningNetwork(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size = 8, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size = 4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18d5f5-1e4f-404d-90ca-695c2474a433",
   "metadata": {},
   "source": [
    "Temporal Difference Learning - Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd7154-fbaf-4529-a593-409127d51eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Adapt to be a Q-learning agent <- Neural network\n",
    "class TD_QLearningAgent(BaseAgent):\n",
    "    def agent_init(self, agent_info={}):\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\")\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\")\n",
    "\n",
    "        self.num_states = agent_info.get(\"num_states\")\n",
    "        self.num_actions = agent_info.get(\"num_actions\")\n",
    "\n",
    "        # initialize the neural network\n",
    "\n",
    "        # This line is drawn from PyTorch documentation\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_net = QLearningNetwork(self.num_actions).to(self.device)\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr = .001, momentum = .9)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        # initialize the agent init state and agent to none\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        tensor = torch.tensor(state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        q_values = self.q_net(tensor)\n",
    "        action = torch.softmax(q_values).item()\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        best_action = np.argmax(self.q_values[state])\n",
    "        self.q_values[self.last_state, self.last_action] = \\\n",
    "                                self.q_values[self.last_state, self.last_action] + self.step_size * (reward + self.discount * \\\n",
    "                                self.q_values[state, best_action] - self.q_values[self.last_state, self.last_action])\n",
    "    \n",
    "        action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        self.q_values[self.last_state, self.last_action] += self.step_size * (reward - self.q_values[self.last_state, self.last_action])\n",
    "\n",
    "    def agent_cleanup(self):        \n",
    "        self.last_state = None\n",
    "        self.last_action = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d22c8-9d3c-418d-8c53-f91950438eb0",
   "metadata": {},
   "source": [
    "Training Agents on Atari - THERE IS A LINK ON THE SLIDES FROM 11/2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6ebff-4073-484f-ac3b-710008fad0b9",
   "metadata": {},
   "source": [
    "https://ale.farama.org/environments/complete_list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74294a0f-d5ac-4cb8-8769-2b3cb291cdd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBreakout\u001b[39m\u001b[38;5;124m\"\u001b[39m,obs_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, repeat_action_probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, full_action_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# get the number of actions\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout\",obs_type = \"rgb\", frame_skip = 1, repeat_action_probability = 0, full_action_space = False)\n",
    "env.reset()\n",
    "\n",
    "# get the number of actions\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# get the actions associated with inputs\n",
    "# for breakout\n",
    "# 0 = Back\n",
    "# 1 = launch\n",
    "# 2 = left\n",
    "# 3 = right\n",
    "meaning = env.unwrapped.get_action_meanings()\n",
    "\n",
    "# for testing \n",
    "obs, reward, terminated, truncated, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659c023-f6e6-4680-afa5-4f44417752f3",
   "metadata": {},
   "source": [
    "Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388778f-f549-4e91-8675-2047bd091ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
