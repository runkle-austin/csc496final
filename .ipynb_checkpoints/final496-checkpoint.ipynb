{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc615f9-37fc-490d-80c2-b3edd8137aa9",
   "metadata": {},
   "source": [
    "Decision Transformer: Reinforcement Learning via Sequence Modeling\n",
    "Recreated by : Austin Runkle, \n",
    "\n",
    "In this project we will be implementing the decision transformer and comparing its preformance\n",
    "to an existing RL model TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3d7b5-271c-4fb3-9bb7-6f679f7fba04",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14b4b9-5c8d-45c0-b9d7-df8ead470e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004c203e-196d-44b1-a44f-08a6dc54a674",
   "metadata": {},
   "source": [
    "Decision Transformer - For Continuous Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5de3b-6897-4018-b4e7-55cdbd0f9223",
   "metadata": {},
   "source": [
    "Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559e7a0-8f1c-41b4-bbd7-481815a26034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ebcbbb-ce71-4d33-97d4-d48b6e66ba2e",
   "metadata": {},
   "source": [
    "Decision Transformer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f4ac4-ecac-4366-8bbb-f2b8de6c2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R, s, a, t: returns -to -go , states , actions , or timesteps\n",
    "# K: context length ( length of each input to DecisionTransformer )\n",
    "# transformer : transformer with causal masking (GPT)\n",
    "# embed_s , embed_a , embed_R : linear embedding layers\n",
    "# embed_t : learned episode positional embedding\n",
    "# pred_a : linear action prediction layer\n",
    "# main model\n",
    "def DecisionTransformer (R , s , a , t ):\n",
    "    # compute embeddings for tokens\n",
    "    pos_embedding = embed_t ( t ) # per - timestep ( note : not per - token )\n",
    "    s_embedding = embed_s ( s ) + pos_embedding\n",
    "    a_embedding = embed_a ( a ) + pos_embedding\n",
    "    R_embedding = embed_R ( R ) + pos_embedding\n",
    "    # interleave tokens as (R_1 , s_1 , a_1 , ... , R_K , s_K )\n",
    "    input_embeds = stack ( R_embedding , s_embedding , a_embedding )\n",
    "    # use transformer to get hidden states\n",
    "    hidden_states = transformer ( input_embeds = input_embeds )\n",
    "    # select hidden states for action prediction tokens\n",
    "    a_hidden = unstack ( hidden_states ). actions\n",
    "    # predict action\n",
    "    return pred_a ( a_hidden )\n",
    "# training loop\n",
    "for (R , s , a , t ) in dataloader : # dims : ( batch_size , K, dim )\n",
    "    a_preds = DecisionTransformer (R , s , a , t )\n",
    "    loss = mean (( a_preds - a )**2) # L2 loss for continuous actions\n",
    "    optimizer . zero_grad (); loss . backward (); optimizer . step ()\n",
    "# evaluation loop\n",
    "target_return = 1 # for instance , expert - level return\n",
    "R , s , a , t , done = [ target_return ] , [ env . reset ()] , [] , [1] , False\n",
    "while not done : # autoregressive generation / sampling\n",
    "    # sample next action\n",
    "    action = DecisionTransformer (R , s , a , t )[ -1] # for cts actions\n",
    "    new_s , r , done , _ = env . step ( action )\n",
    "    # append new tokens to sequence\n",
    "    R = R + [ R [ -1] - r] # decrement returns -to -go with reward\n",
    "    s , a , t = s + [ new_s ] , a + [ action ] , t + [ len ( R )]\n",
    "    R , s , a , t = R [ - K :] , ... # only keep context length of K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18d5f5-1e4f-404d-90ca-695c2474a433",
   "metadata": {},
   "source": [
    "Temporal Difference Learning - Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd7154-fbaf-4529-a593-409127d51eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Adapt to be a Q-learning agent <- Neural network\n",
    "class TDAgent(BaseAgent):\n",
    "    def agent_init(self, agent_info={}):\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
    "        self.policy = agent_info.get(\"policy\")\n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\")\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\")\n",
    "        # Initialize an array of zeros that will hold the values.\n",
    "        self.q_values = np.zeros((num_states, num_zeros))\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
    "        self.last_state = state\n",
    "        return action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        self.q_values[self.last_state] = self.values[self.last_state] + self.step_size * (reward + self.discount * \n",
    "                                                                                    self.values[state] - self.values[self.last_state])\n",
    "    \n",
    "        action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n",
    "        self.last_state = state\n",
    "    \n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        self.values[self.last_state] += self.step_size * (reward - self.values[self.last_state])\n",
    "\n",
    "    def agent_cleanup(self):        \n",
    "        self.last_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d22c8-9d3c-418d-8c53-f91950438eb0",
   "metadata": {},
   "source": [
    "Training Agents on Atari - THERE IS A LINK ON THE SLIDES FROM 11/2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6ebff-4073-484f-ac3b-710008fad0b9",
   "metadata": {},
   "source": [
    "https://ale.farama.org/environments/complete_list/ <- HERE IT IS, dont forget to delete before submiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74294a0f-d5ac-4cb8-8769-2b3cb291cdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9659c023-f6e6-4680-afa5-4f44417752f3",
   "metadata": {},
   "source": [
    "Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388778f-f549-4e91-8675-2047bd091ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
