{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc615f9-37fc-490d-80c2-b3edd8137aa9",
   "metadata": {},
   "source": [
    "Decision Transformer: Reinforcement Learning via Sequence Modeling\n",
    "Recreated by : Austin Runkle, Fatih Bozdogan\n",
    "\n",
    "In this project we will be implementing the decision transformer and comparing its preformance\n",
    "to an existing RL model TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3d7b5-271c-4fb3-9bb7-6f679f7fba04",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c14b4b9-5c8d-45c0-b9d7-df8ead470e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c203e-196d-44b1-a44f-08a6dc54a674",
   "metadata": {},
   "source": [
    "Decision Transformer - For Continuous Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e209894-249c-4f76-a7d8-bbb1cead8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For continuous action, we use an MSE\n",
    "# https://github.com/kzl/decision-transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5de3b-6897-4018-b4e7-55cdbd0f9223",
   "metadata": {},
   "source": [
    "Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559e7a0-8f1c-41b4-bbd7-481815a26034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06558ae-5496-4bf4-b2c0-e00a78fedfb9",
   "metadata": {},
   "source": [
    "Online v. Offline RL: \n",
    "* Online: Learn from experience\n",
    "* Offline RL: Learn from shown experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebcbbb-ce71-4d33-97d4-d48b6e66ba2e",
   "metadata": {},
   "source": [
    "Decision Transformer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4f4ac4-ecac-4366-8bbb-f2b8de6c2359",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_a ( a_hidden )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (R , s , a , t ) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m : \u001b[38;5;66;03m# dims : ( batch_size , K, dim )\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     a_preds \u001b[38;5;241m=\u001b[39m DecisionTransformer (R , s , a , t )\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mean (( a_preds \u001b[38;5;241m-\u001b[39m a )\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# L2 loss for continuous actions\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# R, s, a, t: returns -to -go , states , actions , or timesteps\n",
    "# K: context length ( length of each input to DecisionTransformer )\n",
    "# transformer : transformer with causal masking (GPT)\n",
    "# embed_s , embed_a , embed_R : linear embedding layers\n",
    "# embed_t : learned episode positional embedding\n",
    "# pred_a : linear action prediction layer\n",
    "# main model\n",
    "def DecisionTransformer (R , s , a , t ):\n",
    "    # compute embeddings for tokens\n",
    "    pos_embedding = embed_t ( t ) # per - timestep ( note : not per - token )\n",
    "    s_embedding = embed_s ( s ) + pos_embedding\n",
    "    a_embedding = embed_a ( a ) + pos_embedding\n",
    "    R_embedding = embed_R ( R ) + pos_embedding\n",
    "    # interleave tokens as (R_1 , s_1 , a_1 , ... , R_K , s_K )\n",
    "    input_embeds = stack ( R_embedding , s_embedding , a_embedding )\n",
    "    # use transformer to get hidden states\n",
    "    hidden_states = transformer ( input_embeds = input_embeds )\n",
    "    # select hidden states for action prediction tokens\n",
    "    a_hidden = unstack ( hidden_states ). actions\n",
    "    # predict action\n",
    "    return pred_a ( a_hidden )\n",
    "# training loop\n",
    "for (R , s , a , t ) in dataloader : # dims : ( batch_size , K, dim )\n",
    "    a_preds = DecisionTransformer (R , s , a , t )\n",
    "    loss = mean (( a_preds - a )**2) # L2 loss for continuous actions\n",
    "    optimizer . zero_grad (); loss . backward (); optimizer . step ()\n",
    "# evaluation loop\n",
    "target_return = 1 # for instance , expert - level return\n",
    "R , s , a , t , done = [ target_return ] , [ env . reset ()] , [] , [1] , False\n",
    "while not done : # autoregressive generation / sampling\n",
    "    # sample next action\n",
    "    action = DecisionTransformer (R , s , a , t )[ -1] # for cts actions\n",
    "    new_s , r , done , _ = env . step ( action )\n",
    "    # append new tokens to sequence\n",
    "    R = R + [ R [ -1] - r] # decrement returns -to -go with reward\n",
    "    s , a , t = s + [ new_s ] , a + [ action ] , t + [ len ( R )]\n",
    "    R , s , a , t = R [ - K :] , ... # only keep context length of K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9c808-a180-4416-b467-440886fcf2b0",
   "metadata": {},
   "source": [
    "Neural Network for Q Learning Atari using convolution neural network\n",
    "https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html <- Building Neural Networks\n",
    "https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html <- training classifier\n",
    "https://arxiv.org/pdf/1312.5602 <- confusion matrix sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e55e9d-9f57-4e6d-b007-388ff5119352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution neural network to work with atari\n",
    "class QLearningNetwork(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size = 8, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size = 4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18d5f5-1e4f-404d-90ca-695c2474a433",
   "metadata": {},
   "source": [
    "Temporal Difference Learning - Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76bd7154-fbaf-4529-a593-409127d51eb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO - Adapt to be a Q-learning agent <- Neural network\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTD_QLearningAgent\u001b[39;00m(\u001b[43mBaseAgent\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent_info\u001b[38;5;241m=\u001b[39m{}):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrand_generator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState(agent_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseAgent' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO - Adapt to be a Q-learning agent <- Neural network\n",
    "class TD_QLearningAgent(BaseAgent):\n",
    "    def agent_init(self, agent_info={}):\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\")\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\")\n",
    "\n",
    "        self.num_states = agent_info.get(\"num_states\")\n",
    "        self.num_actions = agent_info.get(\"num_actions\")\n",
    "\n",
    "        # initialize the neural network\n",
    "\n",
    "        # This line is drawn from PyTorch documentation\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_net = QLearningNetwork(self.num_actions).to(self.device)\n",
    "        self.optimizer = optim.SGD(self.q_net.parameters(), lr = .001, momentum = .9)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # initialize the agent init state and agent to none\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        tensor = torch.tensor(state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        q_values = self.q_net(tensor)\n",
    "        action = torch.argmax(q_values, dim = 1).item()\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        # get the current and next state as tensor\n",
    "        cur_state = torch.tensor(self.last_state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        next_state = torch.tensor(state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "\n",
    "        q_values = self.q_net(cur_state)\n",
    "        next_q = self.q_net(next_state)\n",
    "\n",
    "        loss = self.loss_fn(q_values[0, self.last_action], (reward + self.discount * torch.max(next_q)).detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # e greedy next action\n",
    "        action = torch.argmax(next_q, dim = 1).item()\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        # for agent_end compute just the last action \n",
    "        cur_state = torch.tensor(self.last_state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        q_values = self.q_net(cur_state)\n",
    "        loss = self.loss_fn((q_values[0, self.last_action]), torch.tensor(reward, device = self.device))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def agent_cleanup(self):        \n",
    "        self.last_state = None\n",
    "        self.last_action = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d22c8-9d3c-418d-8c53-f91950438eb0",
   "metadata": {},
   "source": [
    "Training Agents on Atari - THERE IS A LINK ON THE SLIDES FROM 11/2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6ebff-4073-484f-ac3b-710008fad0b9",
   "metadata": {},
   "source": [
    "https://ale.farama.org/environments/complete_list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74294a0f-d5ac-4cb8-8769-2b3cb291cdd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `Breakout` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBreakout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mobs_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_skip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat_action_probability\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_action_space\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get the number of actions\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/miniconda3/envs/ml/lib/python3.12/site-packages/gymnasium/envs/registration.py:740\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/miniconda3/envs/ml/lib/python3.12/site-packages/gymnasium/envs/registration.py:537\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    531\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m     )\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "File \u001b[0;32m~/Desktop/miniconda3/envs/ml/lib/python3.12/site-packages/gymnasium/envs/registration.py:403\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/miniconda3/envs/ml/lib/python3.12/site-packages/gymnasium/envs/registration.py:380\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    377\u001b[0m namespace_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment `Breakout` doesn't exist."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout\",obs_type = \"rgb\", frame_skip = 1, repeat_action_probability = 0, full_action_space = False)\n",
    "env.reset()\n",
    "\n",
    "# get the number of actions\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# get the actions associated with inputs\n",
    "# for breakout\n",
    "# 0 = Back\n",
    "# 1 = launch\n",
    "# 2 = left\n",
    "# 3 = right\n",
    "meaning = env.unwrapped.get_action_meanings()\n",
    "\n",
    "# for testing \n",
    "obs, reward, terminated, truncated, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659c023-f6e6-4680-afa5-4f44417752f3",
   "metadata": {},
   "source": [
    "Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388778f-f549-4e91-8675-2047bd091ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489d64a-a2c6-43ce-9703-d915365698e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99830171-6288-421d-b77b-29cfc3d1b054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
