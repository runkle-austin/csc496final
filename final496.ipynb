{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc615f9-37fc-490d-80c2-b3edd8137aa9",
   "metadata": {},
   "source": [
    "Decision Transformer: Reinforcement Learning via Sequence Modeling\n",
    "Recreated by : Austin Runkle, Fatih Bozdogan, Haocheng Cao\n",
    "\n",
    "In this project we will be implementing the decision transformer and comparing its preformance\n",
    "to an existing RL model TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3d7b5-271c-4fb3-9bb7-6f679f7fba04",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c14b4b9-5c8d-45c0-b9d7-df8ead470e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#import gymnasium as gym\n",
    "#from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c203e-196d-44b1-a44f-08a6dc54a674",
   "metadata": {},
   "source": [
    "Decision Transformer - For Continuous Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cad229",
   "metadata": {},
   "source": [
    "# Decision Transformer (Discrete) — What’s included\n",
    "\n",
    "This block implements a **Decision Transformer for discrete action spaces** (e.g., Atari). It follows the “(rtg, state, previous action)” tokenization and uses **cross-entropy** to predict the next action at every action token.\n",
    "\n",
    "## Tokenization\n",
    "For a context window of length **K**, we create 3 tokens per step:\n",
    "1. `rtg_t`  (return-to-go at step t)\n",
    "2. `s_t`    (state at step t)\n",
    "3. `a_{t-1}` (previous action; use a special START id for t=0)\n",
    "\n",
    "So the full sequence is:\n",
    "`[rtg_0, s_0, a_{-1},  rtg_1, s_1, a_0,  ...,  rtg_{K-1}, s_{K-1}, a_{K-2}]`.\n",
    "\n",
    "At every **action token** position, the model predicts the current action `a_t`.\n",
    "\n",
    "## Modules\n",
    "- **CNNStateEncoder**: encodes stacked Atari frames `(4×84×84)` into a `d_model` vector.\n",
    "- **CausalTransformer**: `nn.TransformerEncoder` with a **causal mask**.\n",
    "- **DecisionTransformerDiscrete**:\n",
    "  - Embeddings for RTG (`Linear(1→d)`), state (CNN or `Linear`), action (`Embedding(num_actions+1, d)` with an extra **START** id), timestep (`Embedding`), and token type (rtg/state/action).\n",
    "  - Interleaves tokens `[rtg, s, a] * K`, applies a causal transformer, then selects hidden states at **action token** positions and projects to action logits.\n",
    "- **step_mask_to_token_mask**: expands a step mask `(B,K)` to a token mask `(B,3K)`.\n",
    "- **compute_dt_loss**: cross-entropy over action tokens, ignoring padded steps.\n",
    "- **compute_rtg**: utility to compute returns-to-go from rewards.\n",
    "- **dt_sample_action**: inference helper that returns the predicted action for the last step of a window.\n",
    "\n",
    "## Shapes (batch-first)\n",
    "- `rtg`:            `(B, K, 1)`\n",
    "- `states`:         pixel `(B, K, 4, 84, 84)` **or** vector `(B, K, state_dim)`\n",
    "- `actions_in`:     `(B, K)` longs — these are `a_{t-1}`, with `-1` at `t=0`\n",
    "- `actions_target`: `(B, K)` longs — labels are `a_t`\n",
    "- `timesteps`:      `(B, K)` longs\n",
    "- `step_mask`:      `(B, K)` bool (True=valid, False=pad)\n",
    "- `attention_mask`: `(B, 3K)` bool (True=keep). Use `step_mask_to_token_mask(step_mask)` to build it.\n",
    "\n",
    "## Training loop (sketch)\n",
    "1. Prepare offline windows from rollouts (or a dataset):\n",
    "   - Compute `rtg` with `compute_rtg(rewards, gamma)`.\n",
    "   - Build `(rtg, states, actions_in, actions_target, timesteps, step_mask)` for each window.\n",
    "2. Forward:\n",
    "   ```python\n",
    "   loss, logits = compute_dt_loss(model, batch)\n",
    "   ```\n",
    "3. Backward:  \n",
    "   ```python\n",
    "   optimizer.zero_grad()\n",
    "   loss.backward()\n",
    "   optimizer.step()\n",
    "   ```\n",
    "\n",
    "## Inference\n",
    "Use ``dt_sample_action`` to **autoregressively** pick the next action given the current window:  \n",
    "```python\n",
    "a = dt_sample_action(model, rtg_seq, state_seq, action_in_seq, timestep_seq)\n",
    "```\n",
    "Make sure the first ``action_in_seq[0] == -1`` (START).\n",
    "\n",
    "## Suggested hyperparameters (starting points)\n",
    "- ``d_model=256``,``n_layers=4``, ``n_heads=4``, ``dropout=0.1``\n",
    "- ``K in [20, 30]``\n",
    "- ``rtg_scale=1000.0``\n",
    "- Optimizer: AdamW with ``lr=1e-4 ~ 3e-4``, ``weight_decay=0.1``\n",
    "- Batch size depends on GPU memory (start with 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5de3b-6897-4018-b4e7-55cdbd0f9223",
   "metadata": {},
   "source": [
    "Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e559e7a0-8f1c-41b4-bbd7-481815a26034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Pixel state encoder (for 4x84x84 Atari stacks)\n",
    "# ------------------------------\n",
    "class CNNStateEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  (B*K, C=4, H=84, W=84)\n",
    "    Output: (B*K, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, d_model=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),           nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),           nn.ReLU(),\n",
    "        )\n",
    "        # For 84x84 input, this yields 64x7x7 after the conv stack\n",
    "        self.fc = nn.Linear(64 * 7 * 7, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B*K, C, H, W) | uint8 or float\n",
    "        x = x / 255.0\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Causal Transformer (GPT-style)\n",
    "# ------------------------------\n",
    "class CausalTransformer(nn.Module):\n",
    "    def __init__(self, d_model=256, n_layers=4, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=4 * d_model, dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        # x: (B, L, d)\n",
    "        # attn_mask: (L, L)  True = block, False = allow\n",
    "        # key_padding_mask: (B, L) True = pad(ignored), False = keep\n",
    "        return self.encoder(x, mask=attn_mask, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "\n",
    "def build_causal_mask(L, device):\n",
    "    \"\"\"\n",
    "    Upper-triangular True (=block future), lower incl. diagonal False (=allow past/self).\n",
    "    Shape: (L, L)\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(L, L, device=device), diagonal=1).bool()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Decision Transformer for DISCRETE actions\n",
    "#    token order per step: [rtg_t, s_t, a_{t-1}], predict action at each action token\n",
    "# ------------------------------\n",
    "class DecisionTransformerDiscrete(nn.Module):\n",
    "    \"\"\"\n",
    "    Works for discrete action spaces (e.g., Atari).\n",
    "    - Pixel states: (B, K, 4, 84, 84) with pixel_inputs=True\n",
    "    - Vector states: set pixel_inputs=False and in_channels=state_dim\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions,\n",
    "        d_model=256,\n",
    "        max_timestep=4096,\n",
    "        n_layers=4,\n",
    "        n_heads=4,\n",
    "        dropout=0.1,\n",
    "        pixel_inputs=True,\n",
    "        in_channels=4,      # if pixel_inputs=False, set to state_dim\n",
    "        rtg_scale=1000.0    # scale RTG to ~[0,1] range for stability\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.d_model = d_model\n",
    "        self.pixel_inputs = pixel_inputs\n",
    "        self.rtg_scale = rtg_scale\n",
    "\n",
    "        if pixel_inputs:\n",
    "            self.state_encoder = CNNStateEncoder(in_channels=in_channels, d_model=d_model)\n",
    "        else:\n",
    "            self.state_proj = nn.Linear(in_channels, d_model)  # in_channels=state_dim\n",
    "\n",
    "        # Action embedding: reserve an extra id for \"START\" (a_{-1})\n",
    "        self.embed_action = nn.Embedding(num_actions + 1, d_model)\n",
    "\n",
    "        # RTG, time, and token-type embeddings\n",
    "        self.embed_rtg  = nn.Linear(1, d_model)\n",
    "        self.embed_time = nn.Embedding(max_timestep, d_model)\n",
    "        # token types: 0=rtg, 1=state, 2=action\n",
    "        self.embed_tokentype = nn.Embedding(3, d_model)\n",
    "\n",
    "        self.transformer = CausalTransformer(\n",
    "            d_model=d_model, n_layers=n_layers, n_heads=n_heads, dropout=dropout\n",
    "        )\n",
    "        self.action_head = nn.Linear(d_model, num_actions)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def _interleave(self, rtg_emb, state_emb, action_emb):\n",
    "        # rtg_emb/state_emb/action_emb: (B, K, d) -> (B, 3K, d)\n",
    "        B, K, D = rtg_emb.shape\n",
    "        seq = torch.stack([rtg_emb, state_emb, action_emb], dim=2)  # (B, K, 3, d)\n",
    "        seq = seq.view(B, K * 3, D)\n",
    "        return seq\n",
    "\n",
    "    def forward(self, rtg, states, actions, timesteps, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          rtg:       (B, K, 1)  float\n",
    "          states:    (B, K, C, H, W)  or (B, K, state_dim)\n",
    "          actions:   (B, K)  long; these are a_{t-1}, with -1 at t=0 (START)\n",
    "          timesteps: (B, K)  long; either window-local [0..K-1] or real env steps\n",
    "          attention_mask (optional): (B, 3K) bool; True=keep, False=pad/ignore\n",
    "\n",
    "        Returns:\n",
    "          logits: (B, K, num_actions) — per-step action logits (predict a_t at each action token)\n",
    "        \"\"\"\n",
    "        B, K = actions.shape\n",
    "        device = actions.device\n",
    "\n",
    "        t_embed = self.embed_time(timesteps)  # (B, K, d)\n",
    "\n",
    "        # 1) RTG embedding (scaled) + time + token type\n",
    "        rtg_in  = rtg / self.rtg_scale\n",
    "        rtg_emb = self.embed_rtg(rtg_in) \\\n",
    "                  + t_embed \\\n",
    "                  + self.embed_tokentype(torch.zeros_like(actions))  # 0=rtg\n",
    "\n",
    "        # 2) State embedding\n",
    "        if self.pixel_inputs:\n",
    "            s = states.view(B * K, *states.shape[2:])   # (B*K, C, H, W)\n",
    "            s_emb = self.state_encoder(s).view(B, K, self.d_model)\n",
    "        else:\n",
    "            s_emb = self.state_proj(states)             # (B, K, d)\n",
    "        s_emb = s_emb \\\n",
    "                + t_embed \\\n",
    "                + self.embed_tokentype(torch.ones_like(actions))     # 1=state\n",
    "\n",
    "        # 3) Action embedding (a_{t-1}); fill -1 with START id = num_actions\n",
    "        a_ids = actions.clone()\n",
    "        a_ids = torch.where(a_ids < 0, torch.full_like(a_ids, self.num_actions), a_ids)\n",
    "        a_emb = self.embed_action(a_ids) \\\n",
    "                + t_embed \\\n",
    "                + self.embed_tokentype(torch.full_like(actions, 2))  # 2=action\n",
    "\n",
    "        # 4) Interleave [rtg, s, a] * K  -> (B, 3K, d)\n",
    "        x = self._interleave(rtg_emb, s_emb, a_emb)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # 5) Causal attention\n",
    "        L = x.size(1)\n",
    "        causal_mask = build_causal_mask(L, device)\n",
    "\n",
    "        # Convert token mask to key_padding_mask if provided\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = ~attention_mask  # True=pad\n",
    "\n",
    "        # 6) Transformer + pick hidden states at action token positions\n",
    "        out = self.transformer(x, attn_mask=causal_mask, key_padding_mask=key_padding_mask)\n",
    "        # action token positions: 2,5,8,... -> 3*i + 2\n",
    "        idx = torch.arange(K, device=device) * 3 + 2\n",
    "        idx = idx.unsqueeze(0).expand(B, -1)  # (B, K)\n",
    "        a_hidden = out.gather(dim=1, index=idx.unsqueeze(-1).expand(B, K, self.d_model))\n",
    "        logits = self.action_head(a_hidden)   # (B, K, num_actions)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Step mask -> token mask (each step expands to 3 tokens)\n",
    "# ------------------------------\n",
    "def step_mask_to_token_mask(step_mask):\n",
    "    \"\"\"\n",
    "    step_mask:  (B, K) bool, True=valid step, False=padding\n",
    "    return:     (B, 3K) bool\n",
    "    \"\"\"\n",
    "    B, K = step_mask.shape\n",
    "    return step_mask.unsqueeze(-1).expand(B, K, 3).reshape(B, K * 3)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Cross-entropy loss over action tokens (ignoring padded steps)\n",
    "# ------------------------------\n",
    "def compute_dt_loss(model, batch, ignore_index=-100):\n",
    "    \"\"\"\n",
    "    batch must contain:\n",
    "      rtg:            (B, K, 1) float\n",
    "      states:         (B, K, C, H, W) or (B, K, state_dim)\n",
    "      actions_in:     (B, K) long  — a_{t-1}, with -1 at t=0\n",
    "      actions_target: (B, K) long  — labels a_t\n",
    "      timesteps:      (B, K) long\n",
    "      step_mask:      (B, K) bool (True=valid, False=pad)\n",
    "    Returns:\n",
    "      loss (scalar), logits (B, K, num_actions)\n",
    "    \"\"\"\n",
    "    rtg       = batch['rtg']\n",
    "    states    = batch['states']\n",
    "    actions_in= batch['actions_in']\n",
    "    targets   = batch['actions_target']\n",
    "    timesteps = batch['timesteps']\n",
    "    step_mask = batch.get('step_mask', torch.ones_like(actions_in, dtype=torch.bool))\n",
    "\n",
    "    token_mask = step_mask_to_token_mask(step_mask)\n",
    "    logits = model(rtg, states, actions_in, timesteps, attention_mask=token_mask)\n",
    "\n",
    "    B, K, A = logits.shape\n",
    "    logits_flat  = logits.reshape(B * K, A)\n",
    "    targets_flat = targets.reshape(B * K)\n",
    "    # ignore padded steps\n",
    "    targets_flat_masked = targets_flat.masked_fill(~step_mask.view(-1), ignore_index)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat_masked, ignore_index=ignore_index)\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Returns-to-go utility\n",
    "# ------------------------------\n",
    "def compute_rtg(rewards, gamma=1.0):\n",
    "    \"\"\"\n",
    "    rewards: (T,) list/tensor\n",
    "    return:  (T, 1) where rtg[t] = sum_{t' >= t} gamma^{t'-t} * r[t']\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(rewards):\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    T = rewards.shape[0]\n",
    "    rtg = torch.zeros(T, dtype=torch.float32)\n",
    "    running = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        running = float(rewards[t]) + gamma * running\n",
    "        rtg[t] = running\n",
    "    return rtg.unsqueeze(-1)  # (T, 1)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 7) Inference: sample the next action from the last step\n",
    "# ------------------------------\n",
    "@torch.no_grad()\n",
    "def dt_sample_action(model, rtg_seq, state_seq, action_in_seq, timestep_seq):\n",
    "    \"\"\"\n",
    "    Single-window input (predict at the final step):\n",
    "      rtg_seq:       (K, 1)\n",
    "      state_seq:     (K, C, H, W) or (K, state_dim)\n",
    "      action_in_seq: (K,) long; index 0 should be -1 (START)\n",
    "      timestep_seq:  (K,) long\n",
    "    Returns:\n",
    "      int: predicted discrete action a_t at the last position\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rtg       = rtg_seq.unsqueeze(0)\n",
    "    states    = state_seq.unsqueeze(0)\n",
    "    actions   = action_in_seq.unsqueeze(0)\n",
    "    timesteps = timestep_seq.unsqueeze(0)\n",
    "    logits = model(rtg, states, actions, timesteps)   # (1, K, A)\n",
    "    return int(torch.argmax(logits[0, -1]).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06558ae-5496-4bf4-b2c0-e00a78fedfb9",
   "metadata": {},
   "source": [
    "Online v. Offline RL: \n",
    "* Online: Learn from experience\n",
    "* Offline RL: Learn from shown experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebcbbb-ce71-4d33-97d4-d48b6e66ba2e",
   "metadata": {},
   "source": [
    "Decision Transformer Function - Provided Primarily from the Decision Transformer Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4f4ac4-ecac-4366-8bbb-f2b8de6c2359",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_a ( a_hidden )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (R , s , a , t ) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m : \u001b[38;5;66;03m# dims : ( batch_size , K, dim )\u001b[39;00m\n\u001b[32m     24\u001b[39m     a_preds = DecisionTransformer (R , s , a , t )\n\u001b[32m     25\u001b[39m     loss = mean (( a_preds - a )**\u001b[32m2\u001b[39m) \u001b[38;5;66;03m# L2 loss for continuous actions\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# R, s, a, t: returns -to -go , states , actions , or timesteps\n",
    "# K: context length ( length of each input to DecisionTransformer )\n",
    "# transformer : transformer with causal masking (GPT)\n",
    "# embed_s , embed_a , embed_R : linear embedding layers\n",
    "# embed_t : learned episode positional embedding\n",
    "# pred_a : linear action prediction layer\n",
    "# main model\n",
    "def DecisionTransformer (R , s , a , t ):\n",
    "    # compute embeddings for tokens\n",
    "    pos_embedding = embed_t ( t ) # per - timestep ( note : not per - token )\n",
    "    s_embedding = embed_s ( s ) + pos_embedding\n",
    "    a_embedding = embed_a ( a ) + pos_embedding\n",
    "    R_embedding = embed_R ( R ) + pos_embedding\n",
    "    # interleave tokens as (R_1 , s_1 , a_1 , ... , R_K , s_K )\n",
    "    input_embeds = stack ( R_embedding , s_embedding , a_embedding )\n",
    "    # use transformer to get hidden states\n",
    "    hidden_states = transformer ( input_embeds = input_embeds )\n",
    "    # select hidden states for action prediction tokens\n",
    "    a_hidden = unstack ( hidden_states ). actions\n",
    "    # predict action\n",
    "    return pred_a ( a_hidden )\n",
    "# training loop\n",
    "for (R , s , a , t ) in dataloader : # dims : ( batch_size , K, dim )\n",
    "    a_preds = DecisionTransformer (R , s , a , t )\n",
    "    loss = mean (( a_preds - a )**2) # L2 loss for continuous actions\n",
    "    optimizer . zero_grad (); loss . backward (); optimizer . step ()\n",
    "# evaluation loop\n",
    "target_return = 1 # for instance , expert - level return\n",
    "R , s , a , t , done = [ target_return ] , [ env . reset ()] , [] , [1] , False\n",
    "while not done : # autoregressive generation / sampling\n",
    "    # sample next action\n",
    "    action = DecisionTransformer (R , s , a , t )[ -1] # for cts actions\n",
    "    new_s , r , done , _ = env . step ( action )\n",
    "    # append new tokens to sequence\n",
    "    R = R + [ R [ -1] - r] # decrement returns -to -go with reward\n",
    "    s , a , t = s + [ new_s ] , a + [ action ] , t + [ len ( R )]\n",
    "    R , s , a , t = R [ - K :] , ... # only keep context length of K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9c808-a180-4416-b467-440886fcf2b0",
   "metadata": {},
   "source": [
    "Neural Network for Q Learning Atari using convolution neural network\n",
    "https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html <- Building Neural Networks\n",
    "https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html <- training classifier\n",
    "https://arxiv.org/pdf/1312.5602 <- confusion matrix sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e55e9d-9f57-4e6d-b007-388ff5119352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution neural network to work with atari\n",
    "class QLearningNetwork(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size = 8, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size = 4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18d5f5-1e4f-404d-90ca-695c2474a433",
   "metadata": {},
   "source": [
    "Temporal Difference Learning - Q learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76bd7154-fbaf-4529-a593-409127d51eb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO - Adapt to be a Q-learning agent <- Neural network\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTD_QLearningAgent\u001b[39;00m(\u001b[43mBaseAgent\u001b[49m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magent_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent_info={}):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mself\u001b[39m.rand_generator = np.random.RandomState(agent_info.get(\u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseAgent' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO - Adapt to be a Q-learning agent <- Neural network\n",
    "class TD_QLearningAgent(BaseAgent):\n",
    "    def agent_init(self, agent_info={}):\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
    "        # Discount factor (gamma) to use in the updates.\n",
    "        self.discount = agent_info.get(\"discount\")\n",
    "        # The learning rate or step size parameter (alpha) to use in updates.\n",
    "        self.step_size = agent_info.get(\"step_size\")\n",
    "\n",
    "        self.num_states = agent_info.get(\"num_states\")\n",
    "        self.num_actions = agent_info.get(\"num_actions\")\n",
    "\n",
    "        # initialize the neural network\n",
    "\n",
    "        # This line is drawn from PyTorch documentation\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_net = QLearningNetwork(self.num_actions).to(self.device)\n",
    "        self.optimizer = optim.SGD(self.q_net.parameters(), lr = .001, momentum = .9)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # initialize the agent init state and agent to none\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        tensor = torch.tensor(state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        q_values = self.q_net(tensor)\n",
    "        action = torch.argmax(q_values, dim = 1).item()\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        # get the current and next state as tensor\n",
    "        cur_state = torch.tensor(self.last_state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        next_state = torch.tensor(state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "\n",
    "        q_values = self.q_net(cur_state)\n",
    "        next_q = self.q_net(next_state)\n",
    "\n",
    "        loss = self.loss_fn(q_values[0, self.last_action], (reward + self.discount * torch.max(next_q)).detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # e greedy next action\n",
    "        action = torch.argmax(next_q, dim = 1).item()\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        # for agent_end compute just the last action \n",
    "        cur_state = torch.tensor(self.last_state, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        q_values = self.q_net(cur_state)\n",
    "        loss = self.loss_fn((q_values[0, self.last_action]), torch.tensor(reward, device = self.device))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def agent_cleanup(self):        \n",
    "        self.last_state = None\n",
    "        self.last_action = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104d86f-86e3-469e-8289-ec418dbacf99",
   "metadata": {},
   "source": [
    "Cliff Walk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2ff34c-1812-4b18-899a-13f2f8a5bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty CliffWalkEnvironment class.\n",
    "class CliffWalkEnvironment:\n",
    "    def __init__(self):\n",
    "        self.reward_state_term = (None, None, None)\n",
    "    \n",
    "    def env_init(self, env_info={}):\n",
    "        reward = None\n",
    "        state = None\n",
    "        termination = None\n",
    "        self.reward_state_term = (reward, state, termination)\n",
    "        self.grid_h = env_info.get(\"grid_height\", 4) \n",
    "        self.grid_w = env_info.get(\"grid_width\", 12)\n",
    "        self.start_loc = (self.grid_h - 1, 0)\n",
    "        self.goal_loc = (self.grid_h - 1, self.grid_w - 1)\n",
    "        self.cliff = [(self.grid_h - 1, i) for i in range(1, (self.grid_w - 1))]\n",
    "\n",
    "    def state(self):\n",
    "        return loc[0] * self.grid_w + loc[1]\n",
    "\n",
    "    def env_start(self):\n",
    "        \"\"\"The first method called when the episode starts, called before the\n",
    "        agent starts.\n",
    "    \n",
    "        Returns:\n",
    "            The first state from the environment.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        # agent_loc will hold the current location of the agent\n",
    "        self.agent_loc = self.start_loc\n",
    "        # state is the one dimensional state representation of the agent location.\n",
    "        state = self.state(self.agent_loc)\n",
    "        termination = False\n",
    "        self.reward_state_term = (reward, state, termination)\n",
    "        return self.reward_state_term[1]\n",
    "\n",
    "    def isInBounds(x, y, width, height):\n",
    "        return 0 <= y < width and 0 <= x < height\n",
    "\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \"\"\"A step taken by the environment.\n",
    "    \n",
    "        Args:\n",
    "            action: The action taken by the agent\n",
    "    \n",
    "        Returns:\n",
    "            (float, state, Boolean): a tuple of the reward, state,\n",
    "                and boolean indicating if it's terminal.\n",
    "        \"\"\"\n",
    "        \n",
    "        x, y = self.agent_loc\n",
    "    \n",
    "        # UP\n",
    "        if action == 0:\n",
    "            x = x - 1\n",
    "            \n",
    "        # LEFT\n",
    "        elif action == 1:\n",
    "            y = y - 1\n",
    "            \n",
    "        # DOWN\n",
    "        elif action == 2:\n",
    "            x = x + 1\n",
    "            \n",
    "        # RIGHT\n",
    "        elif action == 3:\n",
    "            y = y + 1\n",
    "            \n",
    "        else: \n",
    "            raise Exception(str(action) + \" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!\")\n",
    "    \n",
    "        # if the action takes the agent out-of-bounds\n",
    "        # then the agent stays in the same state\n",
    "        if not isInBounds(x, y, self.grid_w, self.grid_h):\n",
    "            x, y = self.agent_loc\n",
    "            \n",
    "        # assign the new location to the environment object\n",
    "        self.agent_loc = (x, y)\n",
    "        \n",
    "        # by default, assume -1 reward per step and that we did not terminate\n",
    "        reward = -1\n",
    "        terminal = False\n",
    "        \n",
    "        # assign the reward and terminal variables \n",
    "        # - if the agent falls off the cliff (don't forget to reset agent location!)\n",
    "        # - if the agent reaches the goal state\n",
    "        \n",
    "        if self.agent_loc in self.cliff:\n",
    "            reward = -100\n",
    "            self.agent_loc = self.start_loc\n",
    "    \n",
    "        if (self.agent_loc == self.goal_loc):\n",
    "            terminal = True\n",
    "    \n",
    "        # update\n",
    "        self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n",
    "        return self.reward_state_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af62c43-6d37-4fdb-82c5-eb6c6ff1f055",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/api/spaces/ <- this is for the wrapper to the grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef115bc6-b6fe-4232-a093-e31e531dffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteEnv:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def sample(self):\n",
    "        return np.random.randint(self.n)\n",
    "\n",
    "class CliffWalk:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # 4 actions\n",
    "        self.action_space = DiscreteSpace(4)\n",
    "        # grid height, grid width\n",
    "        self.observation_space = DiscreteEnv(4 * 12)\n",
    "\n",
    "    def step(self, action):\n",
    "        result = self.env.env_step(action)\n",
    "\n",
    "\n",
    "        reward = result[0]\n",
    "        state = result[0]\n",
    "        terminated = result[2]\n",
    "        \n",
    "        info = {}\n",
    "\n",
    "        return state, reward, terminated, False, info\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.env_start()\n",
    "        info = {}\n",
    "        return state, info\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85ed1905-6e98-4838-87d9-d3b334a2f582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m agent_info.update({\u001b[33m\"\u001b[39m\u001b[33mpolicy\u001b[39m\u001b[33m\"\u001b[39m: policy})\n\u001b[32m     13\u001b[39m true_values_file = \u001b[33m\"\u001b[39m\u001b[33moptimal_policy_value_fn.npy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m _ = \u001b[43mrun_experiment\u001b[49m(env_info, agent_info, num_episodes=\u001b[32m5000\u001b[39m, experiment_name=\u001b[33m\"\u001b[39m\u001b[33mPolicy Evaluation on Optimal Policy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m                    plot_freq=\u001b[32m500\u001b[39m, true_values_file=true_values_file)\n\u001b[32m     17\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'run_experiment' is not defined"
     ]
    }
   ],
   "source": [
    "env_info = {\"grid_height\": 4, \"grid_width\": 12, \"seed\": 0}\n",
    "agent_info = {\"discount\": 1, \"step_size\": 0.01, \"seed\": 0}\n",
    "\n",
    "# The Optimal Policy that strides just along the cliff\n",
    "policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n",
    "policy[36] = [1, 0, 0, 0]\n",
    "for i in range(24, 35):\n",
    "    policy[i] = [0, 0, 0, 1]\n",
    "policy[35] = [0, 0, 1, 0]\n",
    "\n",
    "agent_info.update({\"policy\": policy})\n",
    "\n",
    "true_values_file = \"optimal_policy_value_fn.npy\"\n",
    "_ = run_experiment(env_info, agent_info, num_episodes=5000, experiment_name=\"Policy Evaluation on Optimal Policy\",\n",
    "                   plot_freq=500, true_values_file=true_values_file)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d22c8-9d3c-418d-8c53-f91950438eb0",
   "metadata": {},
   "source": [
    "Training Agents on Atari "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6ebff-4073-484f-ac3b-710008fad0b9",
   "metadata": {},
   "source": [
    "https://ale.farama.org/environments/complete_list/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74294a0f-d5ac-4cb8-8769-2b3cb291cdd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m env = \u001b[43mgym\u001b[49m.make(\u001b[33m\"\u001b[39m\u001b[33mBreakout\u001b[39m\u001b[33m\"\u001b[39m,obs_type = \u001b[33m\"\u001b[39m\u001b[33mrgb\u001b[39m\u001b[33m\"\u001b[39m, frame_skip = \u001b[32m1\u001b[39m, repeat_action_probability = \u001b[32m0\u001b[39m, full_action_space = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      2\u001b[39m env.reset()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# get the number of actions\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout\",obs_type = \"rgb\", frame_skip = 1, repeat_action_probability = 0, full_action_space = False)\n",
    "env.reset()\n",
    "\n",
    "# get the number of actions\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# get the actions associated with inputs\n",
    "# for breakout\n",
    "# 0 = Back\n",
    "# 1 = launch\n",
    "# 2 = left\n",
    "# 3 = right\n",
    "meaning = env.unwrapped.get_action_meanings()\n",
    "\n",
    "# for testing \n",
    "obs, reward, terminated, truncated, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659c023-f6e6-4680-afa5-4f44417752f3",
   "metadata": {},
   "source": [
    "Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388778f-f549-4e91-8675-2047bd091ec7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489d64a-a2c6-43ce-9703-d915365698e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99830171-6288-421d-b77b-29cfc3d1b054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
